{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37069b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "134a55ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "95428b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data-files/products_orc/*.orc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "db081f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  spark.read.format('orc') \\\n",
    "        .load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "401587f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
      "|product_id|product_category_id|        product_name|product_description|product_price|       product_image|\n",
      "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
      "|      1009|                 45|Diamond Fear No E...|                   |       599.99|http://images.acm...|\n",
      "|      1010|                 46|DBX Vector Series...|                   |        19.98|http://images.acm...|\n",
      "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "29e0c8af",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/C:/Users/DevAdmin/AppData/Roaming/Python/Python39/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "23/05/07 16:06:36 INFO SparkContext: Running Spark version 3.3.1\n",
      "23/05/07 16:06:36 INFO ResourceUtils: ==============================================================\n",
      "23/05/07 16:06:36 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/05/07 16:06:36 INFO ResourceUtils: ==============================================================\n",
      "23/05/07 16:06:36 INFO SparkContext: Submitted application: avro.py\n",
      "23/05/07 16:06:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/05/07 16:06:36 INFO ResourceProfile: Limiting resource is cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: C:\\Users\\DevAdmin\\.ivy2\\cache\n",
      "The jars for the packages stored in: C:\\Users\\DevAdmin\\.ivy2\\jars\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-cae35f15-f964-4338-9090-0bf47456257b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.3.1 in central\n",
      "\tfound org.tukaani#xz;1.8 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      ":: resolution report :: resolve 297ms :: artifacts dl 16ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.spark#spark-avro_2.12;3.3.1 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.tukaani#xz;1.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-cae35f15-f964-4338-9090-0bf47456257b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/15ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/07 16:06:36 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/05/07 16:06:36 INFO SecurityManager: Changing view acls to: DevAdmin\n",
      "23/05/07 16:06:36 INFO SecurityManager: Changing modify acls to: DevAdmin\n",
      "23/05/07 16:06:36 INFO SecurityManager: Changing view acls groups to: \n",
      "23/05/07 16:06:36 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/05/07 16:06:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(DevAdmin); groups with view permissions: Set(); users  with modify permissions: Set(DevAdmin); groups with modify permissions: Set()\n",
      "23/05/07 16:06:37 INFO Utils: Successfully started service 'sparkDriver' on port 65055.\n",
      "23/05/07 16:06:37 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/05/07 16:06:37 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/05/07 16:06:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "23/05/07 16:06:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "23/05/07 16:06:37 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/05/07 16:06:37 INFO DiskBlockManager: Created local directory at C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\blockmgr-ec648a19-5e2c-4fad-9c8f-193060d26d72\n",
      "23/05/07 16:06:37 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "23/05/07 16:06:37 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "23/05/07 16:06:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/05/07 16:06:37 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/05/07 16:06:37 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/05/07 16:06:37 INFO Utils: Successfully started service 'SparkUI' on port 4043.\n",
      "23/05/07 16:06:37 INFO SparkContext: Added JAR file:///C:/Users/DevAdmin/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.3.1.jar at spark://MOETD0216913.mshome.net:65055/jars/org.apache.spark_spark-avro_2.12-3.3.1.jar with timestamp 1683461196467\n",
      "23/05/07 16:06:37 INFO SparkContext: Added JAR file:///C:/Users/DevAdmin/.ivy2/jars/org.tukaani_xz-1.8.jar at spark://MOETD0216913.mshome.net:65055/jars/org.tukaani_xz-1.8.jar with timestamp 1683461196467\n",
      "23/05/07 16:06:37 INFO SparkContext: Added JAR file:///C:/Users/DevAdmin/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://MOETD0216913.mshome.net:65055/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1683461196467\n",
      "23/05/07 16:06:37 INFO SparkContext: Added file file:///C:/Users/DevAdmin/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.3.1.jar at file:///C:/Users/DevAdmin/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.3.1.jar with timestamp 1683461196467\n",
      "23/05/07 16:06:37 INFO Utils: Copying C:\\Users\\DevAdmin\\.ivy2\\jars\\org.apache.spark_spark-avro_2.12-3.3.1.jar to C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\\userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321\\org.apache.spark_spark-avro_2.12-3.3.1.jar\n",
      "23/05/07 16:06:38 INFO SparkContext: Added file file:///C:/Users/DevAdmin/.ivy2/jars/org.tukaani_xz-1.8.jar at file:///C:/Users/DevAdmin/.ivy2/jars/org.tukaani_xz-1.8.jar with timestamp 1683461196467\n",
      "23/05/07 16:06:38 INFO Utils: Copying C:\\Users\\DevAdmin\\.ivy2\\jars\\org.tukaani_xz-1.8.jar to C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\\userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321\\org.tukaani_xz-1.8.jar\n",
      "23/05/07 16:06:38 INFO SparkContext: Added file file:///C:/Users/DevAdmin/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///C:/Users/DevAdmin/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1683461196467\n",
      "23/05/07 16:06:38 INFO Utils: Copying C:\\Users\\DevAdmin\\.ivy2\\jars\\org.spark-project.spark_unused-1.0.0.jar to C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\\userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321\\org.spark-project.spark_unused-1.0.0.jar\n",
      "23/05/07 16:06:38 INFO Executor: Starting executor ID driver on host MOETD0216913.mshome.net\n",
      "23/05/07 16:06:38 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "23/05/07 16:06:38 INFO Executor: Fetching file:///C:/Users/DevAdmin/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1683461196467\n",
      "23/05/07 16:06:38 INFO Utils: C:\\Users\\DevAdmin\\.ivy2\\jars\\org.spark-project.spark_unused-1.0.0.jar has been previously copied to C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\\userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321\\org.spark-project.spark_unused-1.0.0.jar\n",
      "23/05/07 16:06:39 INFO Executor: Fetching file:///C:/Users/DevAdmin/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.3.1.jar with timestamp 1683461196467\n",
      "23/05/07 16:06:39 INFO Utils: C:\\Users\\DevAdmin\\.ivy2\\jars\\org.apache.spark_spark-avro_2.12-3.3.1.jar has been previously copied to C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\\userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321\\org.apache.spark_spark-avro_2.12-3.3.1.jar\n",
      "23/05/07 16:06:39 INFO Executor: Fetching file:///C:/Users/DevAdmin/.ivy2/jars/org.tukaani_xz-1.8.jar with timestamp 1683461196467\n",
      "23/05/07 16:06:39 INFO Utils: C:\\Users\\DevAdmin\\.ivy2\\jars\\org.tukaani_xz-1.8.jar has been previously copied to C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\\userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321\\org.tukaani_xz-1.8.jar\n",
      "23/05/07 16:06:39 INFO Executor: Fetching spark://MOETD0216913.mshome.net:65055/jars/org.apache.spark_spark-avro_2.12-3.3.1.jar with timestamp 1683461196467\n",
      "23/05/07 16:06:39 INFO TransportClientFactory: Successfully created connection to MOETD0216913.mshome.net/172.26.112.1:65055 after 46 ms (0 ms spent in bootstraps)\n",
      "23/05/07 16:06:39 INFO Utils: Fetching spark://MOETD0216913.mshome.net:65055/jars/org.apache.spark_spark-avro_2.12-3.3.1.jar to C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\\userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321\\fetchFileTemp1102636723472529763.tmp\n",
      "23/05/07 16:06:39 INFO Utils: C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\\userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321\\fetchFileTemp1102636723472529763.tmp has been previously copied to C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\\userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321\\org.apache.spark_spark-avro_2.12-3.3.1.jar\n",
      "23/05/07 16:06:39 INFO Executor: Adding file:/C:/Users/DevAdmin/AppData/Local/Temp/spark-99d55b24-0125-4425-a91b-db92a0b4ecd6/userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321/org.apache.spark_spark-avro_2.12-3.3.1.jar to class loader\n",
      "23/05/07 16:06:39 INFO Executor: Fetching spark://MOETD0216913.mshome.net:65055/jars/org.tukaani_xz-1.8.jar with timestamp 1683461196467\n",
      "23/05/07 16:06:39 INFO Utils: Fetching spark://MOETD0216913.mshome.net:65055/jars/org.tukaani_xz-1.8.jar to C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\\userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321\\fetchFileTemp16443029102042475149.tmp\n",
      "23/05/07 16:06:39 INFO Utils: C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\\userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321\\fetchFileTemp16443029102042475149.tmp has been previously copied to C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\\userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321\\org.tukaani_xz-1.8.jar\n",
      "23/05/07 16:06:39 INFO Executor: Adding file:/C:/Users/DevAdmin/AppData/Local/Temp/spark-99d55b24-0125-4425-a91b-db92a0b4ecd6/userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321/org.tukaani_xz-1.8.jar to class loader\n",
      "23/05/07 16:06:39 INFO Executor: Fetching spark://MOETD0216913.mshome.net:65055/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1683461196467\n",
      "23/05/07 16:06:39 INFO Utils: Fetching spark://MOETD0216913.mshome.net:65055/jars/org.spark-project.spark_unused-1.0.0.jar to C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\\userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321\\fetchFileTemp9545837142369450553.tmp\n",
      "23/05/07 16:06:39 INFO Utils: C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\\userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321\\fetchFileTemp9545837142369450553.tmp has been previously copied to C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\\userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321\\org.spark-project.spark_unused-1.0.0.jar\n",
      "23/05/07 16:06:40 INFO Executor: Adding file:/C:/Users/DevAdmin/AppData/Local/Temp/spark-99d55b24-0125-4425-a91b-db92a0b4ecd6/userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
      "23/05/07 16:06:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65070.\n",
      "23/05/07 16:06:40 INFO NettyBlockTransferService: Server created on MOETD0216913.mshome.net:65070\n",
      "23/05/07 16:06:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "23/05/07 16:06:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, MOETD0216913.mshome.net, 65070, None)\n",
      "23/05/07 16:06:40 INFO BlockManagerMasterEndpoint: Registering block manager MOETD0216913.mshome.net:65070 with 434.4 MiB RAM, BlockManagerId(driver, MOETD0216913.mshome.net, 65070, None)\n",
      "23/05/07 16:06:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, MOETD0216913.mshome.net, 65070, None)\n",
      "23/05/07 16:06:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, MOETD0216913.mshome.net, 65070, None)\n",
      "23/05/07 16:06:41 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "23/05/07 16:06:41 INFO SharedState: Warehouse path is 'file:/C:/Users/DevAdmin/MohyWorkSpace/data-files/spark-warehouse'.\n",
      "23/05/07 16:06:42 INFO InMemoryFileIndex: It took 53 ms to list leaf files for 4 paths.\n",
      "23/05/07 16:06:45 INFO FileSourceStrategy: Pushed Filters: IsNotNull(product_price),GreaterThan(product_price,1000.0)\n",
      "23/05/07 16:06:45 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(product_price#4),(product_price#4 > 1000.0)\n",
      "23/05/07 16:06:45 INFO FileSourceStrategy: Output Data Schema: struct<product_id: int, product_category_id: int, product_name: string, product_description: string, product_price: float ... 1 more field>\n",
      "23/05/07 16:06:45 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/05/07 16:06:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/05/07 16:06:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/05/07 16:06:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/05/07 16:06:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/05/07 16:06:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/05/07 16:06:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/05/07 16:06:46 INFO CodeGenerator: Code generated in 163.0995 ms\n",
      "23/05/07 16:06:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.1 KiB, free 434.2 MiB)\n",
      "23/05/07 16:06:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)\n",
      "23/05/07 16:06:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on MOETD0216913.mshome.net:65070 (size: 34.5 KiB, free: 434.4 MiB)\n",
      "23/05/07 16:06:46 INFO SparkContext: Created broadcast 0 from parquet at DirectMethodHandleAccessor.java:104\n",
      "23/05/07 16:06:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4208463 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/05/07 16:06:46 INFO SparkContext: Starting job: parquet at DirectMethodHandleAccessor.java:104\n",
      "23/05/07 16:06:46 INFO DAGScheduler: Got job 0 (parquet at DirectMethodHandleAccessor.java:104) with 4 output partitions\n",
      "23/05/07 16:06:46 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at DirectMethodHandleAccessor.java:104)\n",
      "23/05/07 16:06:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/05/07 16:06:46 INFO DAGScheduler: Missing parents: List()\n",
      "23/05/07 16:06:46 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at parquet at DirectMethodHandleAccessor.java:104), which has no missing parents\n",
      "23/05/07 16:06:46 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 217.5 KiB, free 434.0 MiB)\n",
      "23/05/07 16:06:46 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 77.8 KiB, free 433.9 MiB)\n",
      "23/05/07 16:06:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on MOETD0216913.mshome.net:65070 (size: 77.8 KiB, free: 434.3 MiB)\n",
      "23/05/07 16:06:46 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
      "23/05/07 16:06:46 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at parquet at DirectMethodHandleAccessor.java:104) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "23/05/07 16:06:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks resource profile 0\n",
      "23/05/07 16:06:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (MOETD0216913.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\n",
      "23/05/07 16:06:46 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (MOETD0216913.mshome.net, executor driver, partition 1, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\n",
      "23/05/07 16:06:46 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (MOETD0216913.mshome.net, executor driver, partition 2, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\n",
      "23/05/07 16:06:46 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (MOETD0216913.mshome.net, executor driver, partition 3, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\n",
      "23/05/07 16:06:47 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "23/05/07 16:06:47 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
      "23/05/07 16:06:47 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
      "23/05/07 16:06:47 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "23/05/07 16:06:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/05/07 16:06:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/05/07 16:06:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/05/07 16:06:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/05/07 16:06:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/05/07 16:06:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/05/07 16:06:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/05/07 16:06:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/05/07 16:06:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/05/07 16:06:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/05/07 16:06:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/05/07 16:06:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/05/07 16:06:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/05/07 16:06:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/05/07 16:06:47 INFO FileScanRDD: Reading File path: file:///C:/Users/DevAdmin/MohyWorkSpace/data-files/products_avro/part-m-00002.avro, range: 0-14583, partition values: [empty row]\n",
      "23/05/07 16:06:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/05/07 16:06:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/05/07 16:06:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/05/07 16:06:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/05/07 16:06:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/05/07 16:06:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/05/07 16:06:47 INFO FileScanRDD: Reading File path: file:///C:/Users/DevAdmin/MohyWorkSpace/data-files/products_avro/part-m-00001.avro, range: 0-13278, partition values: [empty row]\n",
      "23/05/07 16:06:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/05/07 16:06:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/05/07 16:06:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/05/07 16:06:47 INFO CodecConfig: Compression: SNAPPY\n",
      "23/05/07 16:06:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/05/07 16:06:47 INFO FileScanRDD: Reading File path: file:///C:/Users/DevAdmin/MohyWorkSpace/data-files/products_avro/part-m-00000.avro, range: 0-13238, partition values: [empty row]\n",
      "23/05/07 16:06:47 INFO CodecConfig: Compression: SNAPPY\n",
      "23/05/07 16:06:47 INFO CodeGenerator: Code generated in 51.4377 ms\n",
      "23/05/07 16:06:47 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/05/07 16:06:47 INFO ParquetOutputFormat: Validation is off\n",
      "23/05/07 16:06:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/05/07 16:06:47 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/05/07 16:06:47 INFO CodeGenerator: Code generated in 19.3281 ms\n",
      "23/05/07 16:06:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"product_id\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"product_category_id\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"product_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"product_description\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \"name\" : \"product_price\",\n",
      "    \"type\" : \"float\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"product_image\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional int32 product_id;\n",
      "  optional int32 product_category_id;\n",
      "  optional binary product_name (STRING);\n",
      "  optional binary product_description (STRING);\n",
      "  optional float product_price;\n",
      "  optional binary product_image (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/05/07 16:06:47 INFO CodecConfig: Compression: SNAPPY\n",
      "23/05/07 16:06:47 INFO CodecConfig: Compression: SNAPPY\n",
      "23/05/07 16:06:47 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/05/07 16:06:47 INFO ParquetOutputFormat: Validation is off\n",
      "23/05/07 16:06:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/05/07 16:06:47 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/05/07 16:06:47 INFO CodecConfig: Compression: SNAPPY\n",
      "23/05/07 16:06:47 INFO CodecConfig: Compression: SNAPPY\n",
      "23/05/07 16:06:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"product_id\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"product_category_id\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"product_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"product_description\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"product_price\",\n",
      "    \"type\" : \"float\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"product_image\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional int32 product_id;\n",
      "  optional int32 product_category_id;\n",
      "  optional binary product_name (STRING);\n",
      "  optional binary product_description (STRING);\n",
      "  optional float product_price;\n",
      "  optional binary product_image (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/05/07 16:06:47 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/05/07 16:06:47 INFO ParquetOutputFormat: Validation is off\n",
      "23/05/07 16:06:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/05/07 16:06:47 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/05/07 16:06:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"product_id\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"product_category_id\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"product_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"product_description\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"product_price\",\n",
      "    \"type\" : \"float\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"product_image\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional int32 product_id;\n",
      "  optional int32 product_category_id;\n",
      "  optional binary product_name (STRING);\n",
      "  optional binary product_description (STRING);\n",
      "  optional float product_price;\n",
      "  optional binary product_image (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/05/07 16:06:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202305071606461046631801837380963_0000_m_000001_1\n",
      "23/05/07 16:06:47 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "23/05/07 16:06:47 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "23/05/07 16:06:47 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2536 bytes result sent to driver\n",
      "23/05/07 16:06:47 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "23/05/07 16:06:47 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 685 ms on MOETD0216913.mshome.net (executor driver) (1/4)\n",
      "23/05/07 16:06:48 INFO FileScanRDD: Reading File path: file:///C:/Users/DevAdmin/MohyWorkSpace/data-files/products_avro/part-m-00003.avro, range: 0-15539, partition values: [empty row]\n",
      "23/05/07 16:06:48 INFO FileOutputCommitter: Saved output of task 'attempt_202305071606461442012873072798699_0000_m_000000_0' to file:/C:/Users/DevAdmin/MohyWorkSpace/Retail_Proj/result/scenario5/solution/_temporary/0/task_202305071606461442012873072798699_0000_m_000000\n",
      "23/05/07 16:06:48 INFO FileOutputCommitter: Saved output of task 'attempt_202305071606463443030804714980472_0000_m_000002_2' to file:/C:/Users/DevAdmin/MohyWorkSpace/Retail_Proj/result/scenario5/solution/_temporary/0/task_202305071606463443030804714980472_0000_m_000002\n",
      "23/05/07 16:06:48 INFO SparkHadoopMapRedUtil: attempt_202305071606461442012873072798699_0000_m_000000_0: Committed. Elapsed time: 4 ms.\n",
      "23/05/07 16:06:48 INFO SparkHadoopMapRedUtil: attempt_202305071606463443030804714980472_0000_m_000002_2: Committed. Elapsed time: 5 ms.\n",
      "23/05/07 16:06:48 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2622 bytes result sent to driver\n",
      "23/05/07 16:06:48 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 2622 bytes result sent to driver\n",
      "23/05/07 16:06:48 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1421 ms on MOETD0216913.mshome.net (executor driver) (2/4)\n",
      "23/05/07 16:06:48 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 1391 ms on MOETD0216913.mshome.net (executor driver) (3/4)\n",
      "23/05/07 16:06:48 INFO FileOutputCommitter: Saved output of task 'attempt_202305071606467405319538237066056_0000_m_000003_3' to file:/C:/Users/DevAdmin/MohyWorkSpace/Retail_Proj/result/scenario5/solution/_temporary/0/task_202305071606467405319538237066056_0000_m_000003\n",
      "23/05/07 16:06:48 INFO SparkHadoopMapRedUtil: attempt_202305071606467405319538237066056_0000_m_000003_3: Committed. Elapsed time: 29 ms.\n",
      "23/05/07 16:06:48 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2622 bytes result sent to driver\n",
      "23/05/07 16:06:48 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 1407 ms on MOETD0216913.mshome.net (executor driver) (4/4)\n",
      "23/05/07 16:06:48 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "23/05/07 16:06:48 INFO DAGScheduler: ResultStage 0 (parquet at DirectMethodHandleAccessor.java:104) finished in 1.594 s\n",
      "23/05/07 16:06:48 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/05/07 16:06:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "23/05/07 16:06:48 INFO DAGScheduler: Job 0 finished: parquet at DirectMethodHandleAccessor.java:104, took 1.666828 s\n",
      "23/05/07 16:06:48 INFO FileFormatWriter: Start to commit write Job b7225694-7a5d-4e21-82a5-4d7e1aa2b685.\n",
      "23/05/07 16:06:48 INFO FileFormatWriter: Write Job b7225694-7a5d-4e21-82a5-4d7e1aa2b685 committed. Elapsed time: 42 ms.\n",
      "23/05/07 16:06:48 INFO FileFormatWriter: Finished processing stats for write job b7225694-7a5d-4e21-82a5-4d7e1aa2b685.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "done\n",
      "23/05/07 16:06:48 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "23/05/07 16:06:48 INFO SparkUI: Stopped Spark web UI at http://MOETD0216913.mshome.net:4043\n",
      "23/05/07 16:06:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "23/05/07 16:06:48 INFO MemoryStore: MemoryStore cleared\n",
      "23/05/07 16:06:48 INFO BlockManager: BlockManager stopped\n",
      "23/05/07 16:06:48 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "23/05/07 16:06:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "23/05/07 16:06:48 WARN SparkEnv: Exception while deleting Spark temp dir: C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\\userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321\n",
      "java.io.IOException: Failed to delete: C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\\userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321\\org.tukaani_xz-1.8.jar\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1206)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:108)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2140)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2140)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:660)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:577)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1623)\n",
      "23/05/07 16:06:48 INFO SparkContext: Successfully stopped SparkContext\n",
      "23/05/07 16:06:48 INFO ShutdownHookManager: Shutdown hook called\n",
      "23/05/07 16:06:48 INFO ShutdownHookManager: Deleting directory C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\n",
      "23/05/07 16:06:48 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\n",
      "java.io.IOException: Failed to delete: C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\\userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321\\org.tukaani_xz-1.8.jar\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1206)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:577)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1623)\n",
      "23/05/07 16:06:48 INFO ShutdownHookManager: Deleting directory C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\\pyspark-303ca75e-e9ba-48c5-86d3-843bfb4c0815\n",
      "23/05/07 16:06:48 INFO ShutdownHookManager: Deleting directory C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-4bd75db1-7611-4dea-b596-2e17489bdd0e\n",
      "23/05/07 16:06:48 INFO ShutdownHookManager: Deleting directory C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\\userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321\n",
      "23/05/07 16:06:48 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\\userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321\n",
      "java.io.IOException: Failed to delete: C:\\Users\\DevAdmin\\AppData\\Local\\Temp\\spark-99d55b24-0125-4425-a91b-db92a0b4ecd6\\userFiles-b7583a55-bc1f-4439-8ecc-b18e8eaac321\\org.tukaani_xz-1.8.jar\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1206)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:577)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1623)\n"
     ]
    }
   ],
   "source": [
    "!spark-submit --packages org.apache.spark:spark-avro_2.12:3.3.1 avro_5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "ef6e93da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
      "|product_id|product_category_id|        product_name|product_description|product_price|       product_image|\n",
      "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
      "|      1048|                 47|Spalding Beast 60...|                   |      1099.99|http://images.acm...|\n",
      "|        66|                  4|  SOLE F85 Treadmill|                   |      1799.99|http://images.acm...|\n",
      "|       199|                 10|  SOLE F85 Treadmill|                   |      1799.99|http://images.acm...|\n",
      "|       208|                 10| SOLE E35 Elliptical|                   |      1999.99|http://images.acm...|\n",
      "|       496|                 22|  SOLE F85 Treadmill|                   |      1799.99|http://images.acm...|\n",
      "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.product_price > 1000.0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74c1fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_5 = 'result/scenario5/solution/*.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e48ced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 =  spark.read.format('parquet') \\\n",
    "        .load(path_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b9e4a31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------+\n",
      "|product_id|        product_name|product_price|\n",
      "+----------+--------------------+-------------+\n",
      "|      1048|Spalding Beast 60...|      1099.99|\n",
      "|        66|  SOLE F85 Treadmill|      1799.99|\n",
      "|       199|  SOLE F85 Treadmill|      1799.99|\n",
      "|       208| SOLE E35 Elliptical|      1999.99|\n",
      "|       496|  SOLE F85 Treadmill|      1799.99|\n",
      "+----------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_5.select('product_id', 'product_name', 'product_price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed16d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-submit --packages org.apache.spark:spark-avro_2.12:3.3.1 avro_6.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "81519ddf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------------------+-------------------+-------------+--------------------+\n",
      "|product_id|product_category_id|      product_name|product_description|product_price|       product_image|\n",
      "+----------+-------------------+------------------+-------------------+-------------+--------------------+\n",
      "|        66|                  4|SOLE F85 Treadmill|                   |      1799.99|http://images.acm...|\n",
      "|       199|                 10|SOLE F85 Treadmill|                   |      1799.99|http://images.acm...|\n",
      "|       496|                 22|SOLE F85 Treadmill|                   |      1799.99|http://images.acm...|\n",
      "+----------+-------------------+------------------+-------------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.product_price > 1000.0).filter(F.col('product_name').like(\"%Treadmill%\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f87bb6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_6 = 'result/scenario6/solution/*.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "41666431",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_6 =  spark.read.format('parquet')\\\n",
    "        .load(path_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "da2a8301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-------------+\n",
      "|product_id|      product_name|product_price|\n",
      "+----------+------------------+-------------+\n",
      "|        66|SOLE F85 Treadmill|      1799.99|\n",
      "|       199|SOLE F85 Treadmill|      1799.99|\n",
      "|       496|SOLE F85 Treadmill|      1799.99|\n",
      "+----------+------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_6.select('product_id', 'product_name', 'product_price').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
